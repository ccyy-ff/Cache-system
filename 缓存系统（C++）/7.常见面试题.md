# 7.常见面试题

这里的答案只是抛转引玉，答案大家可以根据自己的理解整理。

部分题目参考：<https://blog.csdn.net/qq_40374604/article/details/128123061>

注意：部分相关解答可能跟项目本身有些不符合，欢迎在星球项目答疑群提出

#### 你的缓存是什么？你的缓存机制是如何设计的？它用来缓存什么东西？具体在程序中用于哪些场景？

答：缓存是将高频访问的数据暂存到内存中，是加速数据访问的存储，降低延迟，提高吞吐量。

采用LRU，LFU等策略去实现缓存的。

用于缓存一些频繁访问的数据，比如网页等文件资源、数据库的热点数据等。

比如客户端向服务端请求某项文件资源时，服务端程序需要先将该文件资源从磁盘中读出到内存缓冲区中，然后再将其从内存缓冲区中发送给客户端。

如果有了缓存机制，对于已经缓存了的文件，在客户端请求后，服务端就不用再向磁盘读取了，可以从缓存中直接取出文件传输给客户端。避免了读取磁盘带来的开销。高并发场景下频繁读取磁盘也会造成较大的性能开销。

#### 你用缓存机制缓存了数据库数据吗？

* 缓存是可以用于缓存数据库的数据的，因为数据库访问（特别是复杂查询）通常非常慢
* 数据变更频率较低，但访问频率高
* 如果数据是“热点数据”，就更适合缓存

#### 什么是缓存？

答：缓存，就是数据交换的缓冲区，是一种用于临时存储数据的高效存储机制，其主要目的是加快访问速度、减轻后台系统压力，从而提升整体性能。我们平时说的缓存大多是指内存。目的是，把读写速度慢的介质的数据保存在读写速度快的介质中（这里的快与慢是相对概念），从而提高读写速度，减少时间消耗。例如：

* CPU高速缓存：告诉缓存的读写速度远高于内存。
  * CPU读数据时，如果在高速缓存中找到所需数据，就不需要读内存
  * CPU写数据时，先写到高速缓存，再写回内存。
* 磁盘缓存：磁盘缓存其实就把常用的磁盘数据保存在内存中，内存读写速度也是远高于磁盘的。
  * 读数据时从内存中读取
  * 写数据时，可先写回内存，定时或定量写回到磁盘，或者是同步写回。

#### 请说说有哪些缓存算法？是否能手写一下LRU代码的实现？

缓存算法中，比较常见的如下：

* FIFO(先进先出)
* LRU(最近最少使用)
* LFU(最不经常使用)
* ARC(自适应替换)

LRU代码实现如下：

```cpp
#include <unordered_map>
#include <list>
#include <iostream>

class LRUCache {
private:
    struct CacheNode {
        int key;
        int value;
        CacheNode(int k, int v) : key(k), value(v) {}
    };

    int capacity;
    std::list<CacheNode> cacheList; // 双向链表，存储缓存数据
    std::unordered_map<int, std::list<CacheNode>::iterator> cacheMap; // 哈希表，存储键和对应在双向链表中的迭代器

public:
    LRUCache(int capacity) : capacity(capacity) {}

    int get(int key) {
        auto it = cacheMap.find(key);
        if (it == cacheMap.end()) {
            return -1; // 未找到
        }
        // 将访问的节点移动到双向链表的头部
        cacheList.splice(cacheList.begin(), cacheList, it->second);
        return it->second->value;
    }

    void put(int key, int value) {
        auto it = cacheMap.find(key);
        if (it != cacheMap.end()) {
            // 如果键已存在，更新值，并将其移动到双向链表的头部
            it->second->value = value;
            cacheList.splice(cacheList.begin(), cacheList, it->second);
            return;
        }

        if (cacheMap.size() == capacity) {
            // 如果缓存已满，移除双向链表的尾节点
            auto last = cacheList.back();
            cacheMap.erase(last.key);
            cacheList.pop_back();
        }

        // 添加新节点到双向链表的头部
        cacheList.emplace_front(key, value);
        cacheMap[key] = cacheList.begin();
    }
};

int main() {
    LRUCache cache(2);
    cache.put(1, 1);
    cache.put(2, 2);
    std::cout << cache.get(1) << std::endl; // 返回 1
    cache.put(3, 3);                        // 淘汰键2
    std::cout << cache.get(2) << std::endl; // 返回 -1 (未找到)
    cache.put(4, 4);                        // 淘汰键1
    std::cout << cache.get(1) << std::endl; // 返回 -1 (未找到)
    std::cout << cache.get(3) << std::endl; // 返回 3
    std::cout << cache.get(4) << std::endl; // 返回 4
    return 0;
}
```

#### 常见的缓存工具和框架有哪些？

在C++开发中，常见的缓存工具和框架如下：

* Boost.Cache：提供灵活的缓存管理，支持LRU和LFU等策略。
* Redis：支持多种数据结构的分布式缓存。也是最主流最常用的。
* Intel TBB Cache：提供线程安全的缓存实现，支持并发访问。

#### 为什么需要实现这个项目？使用缓存的目标是什么？

**思路**：阐述高并发场景下缓存的意义，例如提升性能、降低数据库压力、优化用户体验等。

#### 多线程下如何保证线程安全？你使用了什么技术或方法来实现线程安全？

**思路**：

* 使用 `std::mutex`、`std::shared_mutex` 等同步机制避免数据竞争。
* 如果有分片优化，可以提到分片缓存降低锁粒度，减少锁竞争。
* 使用原子操作（如 `std::atomic`）优化性能。

#### 实现了哪些缓存策略？如何选择？各自的优缺点是什么？

**思路**：

* 说明实现了 LRU、LFU、ARC 策略及其优化：
  * **LRU 分片**：减少并发访问的锁冲突。
  * **LFU 分片**：降低访问频率更新的竞争。
  * **ARC**：动态平衡命中率和访问频率的需求。
* 对比这些策略的应用场景和缺点。

#### 如何存储缓存数据？数据结构是如何设计的？

**思路**：

* 提到使用 `std::unordered_map`（哈希表）作为快速查找的基础。
* 使用双向链表（如 `std::list`）维护访问顺序，实现 LRU 或 LFU 的高效淘汰。

#### 每种缓存策略的时间复杂度和空间复杂度分别是多少？

**思路**：

* LRU：O(1) 查找、O(1) 插入/删除。
* LFU：O(log 1) 更新频率。
* ARC：动态调整需要额外的复杂性分析。

#### 你对 LRU 和 LFU 策略做了哪些优化？如何验证这些优化的效果？

**思路**：

* LRU 分片：减少锁冲突。
* LRU-k：解决热点数据被冷数据挤出问题。
* LFU 增加最大平均访问频率：避免历史热点长期占用缓存。

#### 多线程高并发下，如何优化缓存的性能？

**思路**：

* 缓存分片：每个分片使用单独的锁，降低锁粒度。
* 读写分离：大量读操作可使用 `std::shared_mutex` 提升性能。
* 非阻塞操作：可以提到 CAS（Compare-And-Swap）优化。

#### 如何评估缓存的性能？测试指标有哪些？

**思路**：

* 测试指标包括命中率、吞吐量、延迟。
* 通过压测工具（如 Webbench、ab）模拟高并发场景，验证性能是否达标。

#### 问题：如何选择锁机制？为什么要降低锁的粒度？

**思路：**

* 锁的粒度越小，锁冲突越低，但管理复杂度增加。
* 使用读写锁时，读多写少场景下可以显著提高性能。

#### 如何同时支持 LRU、LFU 和 ARC 策略？用户如何选择不同的策略？

**思路**：

* 提供抽象基类或接口，设计多种策略的实现。
* 用户可以通过配置或运行时参数选择不同的策略。

#### 如果需要新增一种缓存策略，如何设计？

**思路**：

* 基于面向对象编程的思想，新增策略只需继承接口并实现。

#### 如何应对缓存穿透、缓存击穿和缓存雪崩问题？

**思路**：

* **缓存穿透**：使用布隆过滤器过滤无效查询。
* **缓存击穿**：对热点数据加锁，避免缓存失效后高并发访问后端存储。
* **缓存雪崩**：错开缓存过期时间，或设置合理的回源策略。

#### Redis有哪些淘汰策略

Redis的淘汰策略有以下几种：

1. LRU算法：当内存不足以容纳新写入数据时，移除缓存中最近最少访问的数据
2. LFU算法：当内存不足以容纳新写入数据时，移除缓存中最不经常访问的数据
3. FIFO算法：最早放入缓存的数据最先被删除
4. Random算法：随机移除某个键

当涉及到设置了过期时间的数据时，还有以下策略：

1. volatile-lru：从设置了过期时间的数据中选择最近最少使用的数据淘汰。
2. volatile-lfu：从设置了过期时间的数据中选择最不经常访问的数据淘汰。
3. volatile-random：从设置了过期时间的数据中随机选择数据淘汰。
4. volatile-lfu：从设置了过期时间的数据中选择过期时间最近的键淘汰。

#### 什么是缓存污染，你是如何减少这个问题的？

**思路：**

* **缓存污染定义**：指不重要的数据占据了缓存空间，从而挤出更重要的、经常访问的数据。
* **解决措施**：
  * 使用 **LRU-k**：缓存只淘汰短期未访问的数据，防止热点数据被冷门数据挤出。
  * 增加缓存淘汰策略的灵活性，例如结合访问频率和时间综合判断。

#### 你是如何实现缓存分片的？

**思路：**

* 将缓存划分成多个片段，每个片段有独立的数据结构和锁。
* 根据键值的 **哈希值** 确定对应的分片，从而减少锁争用。
* 每个分片独立管理内存空间和替换策略，以便最大限度优化并发性能。

#### 在C++中，原子操作是如何实现线程安全的？

**思路：**

* 使用 **std::atomic** 提供的操作，例如 `std::atomic<int>`，避免因竞态条件导致数据不一致。
* 底层实现：通过硬件提供的 **原子指令**（如 CPU 的 CAS）来确保操作的不可分割性。
* 优势：无需加锁，减少线程之间的阻塞，提高性能。

#### 你是如何动态调整ARC策略中的权重比例的？

**思路：**

* 根据实时监控的命中率（命中频率 vs. 未命中频率）进行调整。
* 逻辑实现：
  * **LRU 部分权重**：针对短期热点数据。
  * **LFU 部分权重**：针对长期热点数据。
  * 动态调整：命中率下降时，增大 LFU 权重（防止频繁访问的数据被替换）；反之增大 LRU 权重。
* 示例：实现一个自适应机制，通过调节阈值动态切换缓存分区的占比。

#### 在设计高并发缓存系统时，你考虑了哪些性能指标？

**思路：**

* **响应速度**：缓存访问的延迟应尽可能低（如纳秒级）。
* **命中率**：缓存被访问时，命中次数占总访问次数的比例。
* **吞吐量**：系统每秒可以处理的请求数量。
* **资源利用率**：内存占用、CPU 消耗，以及网络 I/O 等的效率。
* **扩展性**：是否可以轻松扩展缓存容量或支持更高并发量。

#### 你是如何优化缓存系统以提高高并发场景下的响应速度的？

**思路：**

* 使用 **缓存分片**，减少全局锁的使用。
* 优化替换策略，减少复杂度。
* 利用 **本地性原则**（如将常用缓存分配在线程本地存储中）。
* 调整数据结构，例如使用 **哈希链表** 或 **跳表** 以减少查询时间复杂度。

#### 你是如何确保在高并发环境下数据的一致性的？

思路：

* 使用 **细粒度锁**：只锁定需要操作的部分数据，而不是全局锁定。
* 使用原子操作确保对共享数据的访问安全。

#### 在C++中，你是如何管理内存的？

思路：

* 使用 **智能指针**：`std::shared_ptr` 和 `std::unique_ptr` 自动管理对象生命周期，避免内存泄漏。
* 使用 **RAII** 原则：在构造时分配资源，析构时自动释放资源。
* 定期清理：缓存中淘汰的对象及时清除，以释放内存。
* 预分配内存池：减少频繁的内存分配操作，优化性能。

#### 你是如何实现缓存的预热功能的？

思路：

* **预加载策略**：在系统启动时，分析历史数据，加载常用或热点数据到缓存中。
* **实现方式**：提供接口将静态数据加载到缓存。例如：

```cpp
复制代码
cache.put("key", "value"); // 手动加载
```

* 优势：减少启动后首次访问的延迟，提高用户体验。

#### 在实现缓存分片时，你是如何确定分片数量的？

思路：

* 根据系统的 **CPU 核心数**，设置分片数量与核心数一致，利用多核性能。
* 根据预期的 **并发量** 和 **负载**，通过压力测试确定分片数量，以确保负载均衡。
* 分片数与哈希函数结合，避免数据分布不均。

#### 你是如何确保缓存系统在面对突发流量时的稳定性的？

**思路：**

* **负载均衡**：通过多个缓存节点分担请求压力。
* **熔断机制**：当缓存压力过大时，临时降级，直接返回默认值或空值。
* \*\*提前扩容：\*\*在高并发场景下，动态扩展缓存容量或缓存节点。


> 更新: 2025-08-09 22:57:07  
> 原文: <https://www.yuque.com/chengxuyuancarl/ctp2tl/mxoq7t6gsil8p40o>